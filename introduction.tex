\tikzset{ font={\fontsize{4pt}{5}\selectfont}}

\IEEEPARstart{A}{utomatic} activity recognition in video
an intensely researched area in computer vision due to its wide range of real-world applications in
sports, health care, surveillance, robot vision, and human-computer interaction.
Furthermore, the rapid growth of digital video data demands automatic
classification and indexing of videos. Despite the increased interest, state-of-the-art
systems are still far from human-level performance, in contrast to the success in image classification \cite{girshick2014rich, krizhevsky2012imagenet}. This is partially due to the complex intra-class variations in videos, some obvious causes being the view point, background
clutter, high dimensionality of data, lack of large data sets, and low resolution.

Despite these reasons more-or-less affecting automatic image classification, it has been quite successful in recent years,
largely owing to the rise of deep learning techniques. This is not the case in video classification, although deep learning is
starting to be widely applied. Therefore, it is worthwhile investigating
\hl{what is holding back video classification}. In this study, we address three key
areas: exploiting the underlying dynamics of sub-events for high-level action recognition, crafting
self-explanatory, independent static and motion features---in which, motion features should capture micro-level actions
of each actor or object independently, such as arm or leg movements---, and optimum fusing of static and motion features for better accuracy.
%These three aspects are briefly explained next.

A complex activity typically comprises several \hl{sub activities}.
The existing approaches try to classify a video treating it as a
single, high-level activity \cite{wang2011action, wang2013action, simonyan2014two, 7486474}.
As the action becomes complex, the behavior and the
temporal evolution of its underlying sub-events become complicated. For example,
cooking may involve sub-events: cutting, turning on the cooker, and stirring. It may not always preserve this
same lower order for the same action class. Instead, they may contain
a higher-order temporal relationship among them. For example, turning on the cooker and cutting
may appear in reverse order in another video in the same class.
Therefore, this temporal pattern of sub-events is not easy to capture through a simple
time series analysis. These patterns can only be identified by observing a large number
of examples, using a system which has an \hl{infinite dynamic response}. Therefore, it is important to
model this higher-order temporal
progression, and capture temporal dynamics of these sub-events for better recognition of
complex activities.

In contrast to image classification, the information contained in videos
are not in a single \hl{domain}. Both the motion patterns of the actors and objects, as well as the static
 information---such as, background and still objects that the actors interact with---are important for determining an action.
For example, the body movements of a group of people fighting may closely relate to
the body movements of a sports event, e.g., wrestling. In such a case, it is tough to distinguish between the two
activities solely by looking at the motion patterns. Inspecting the background setting
and objects is crucial in such a scenario. Therefore, it is necessary to engineer powerful features
from both motion and static domains. In addition, these features must be complementary, and one feature domain should
not influence the other domain. In other words, they should be self-explanatory, and mutually exclusive, as much as possible.
Also, these motion features should be able to capture
activities of each actor independently. If so, the distributional properties of these actions, over short
and long durations, can be used to identify high level actions. In the proposed method, we craft static and motion features to have this property.

Furthermore, how to optimally combine or fuse these motion and static descriptors remains a challenge for three key reasons: both static and motion information
provide cues regarding an action in a video, the contribution ratio from each domain affects the final recognition accuracy,
and optimum contribution ratio of static and motion information may depend on the data set.
The combined descriptor should contain the essence of both domains, and should not have a
negative influence on each other. Recently, there has been attempts to answer this question \cite{7486474},\cite{simonyan2014two}. However,
these existing methodologies lack the insight in to how much this ratio affects the final accuracy, and has no control over this ratio.
One major requirement of the fusion method is to have a high-level intuitive interpretation on how much contribution each domain provides to the
final descriptor, and have the ability to control level of contribution. This ability makes it possible
to deeply investigate the optimum contribution of each domain for a better accuracy. In this study, we investigate this factor
extensively.

In this work, we focus on video activity recognition using both static and motion information. We address three problem areas: crafting \hl{mutually exclusive} static and motion features, optimal fusion of the crafted features, and
modeling temporal dynamics of sub-activities for high-level activity recognition. In order to examine the temporal evolution
of sub-activities subsequently, we create video segments with constant overlapping durations. Afterwards, we combine static and motion
descriptors to represent each segment. We propose \textit{motion tubes},
a dense trajectory \cite{wang2011action} based tracking mechanism, to identify and track candidate moving areas
separately. This enables independent modeling of the activities in each moving area.
In order to capture motion patterns, we use histogram oriented optic flows (HOOF) \cite{chaudhry2009histograms}
inside motion tubes. Then we apply a bag-of-words (BoW) method on the generated
features to capture the distributional properties of micro actions, such as body movements, and create high level, discriminative motion features.

Inspired by the power  of object recognition of convolutional neural networks (CNNs), we create a seven-layer deep
CNN, and pre-train it on the popular ImageNet data set \cite{deng2012imagenet}.
Afterwards, we use this trained CNN to create deep features, to synthesize static descriptors.
Then, using a computationally efficient, yet powerful, mathematical model, we fuse static and motion feature vectors. We propose three such novel methods in this paper: based on
Cholesky decomposition, variance ratio of motion and static vectors, and principal components analysis (PCA). The Choleskey decomposition based model provides the ability to
precisely control the contribution of static and motion domains to the final fused descriptor. Using this intuition, we investigate the
optimum contribution of each domain, experimentally.  The variance ratio based method also provides us this ability, and additionally, lets us
obtain the optimum contribution ratio mathematically. We show that the optimum contribution ratio obtained experimentally using the Choleskey based method,
matches with the ratio obtained mathematically from the variance based method. Furthermore, we show that this optimum contribution may
vary depending on the data set, and affects the final accuracy significantly.
%The PCA-based method also provides good results.


In order to model the temporal progression of sub events, we feed the fused vectors to
a long short-term memory (LSTM) network. The LSTM network discovers the underlying temporal patterns of the sub events, and classifies high level actions.
We feed the same vectors to a classifier which does not capture temporal dynamics to show that modeling temporal progression of sub events indeed contribute for a better
accuracy. We carry out our experiments on the two popular action data sets, UCF-11 \cite{liu2009recognizing}
and Hollywood2 \cite{marszalek2009actions}. Our results surpass existing  state-of-the-art
results on these data sets, to the best of our knowledge.

The key contributions of this paper are as follows:

 \begin{itemize}
  \item We propose an end-to-end system, which  extracts both static and motion information, fuses, and models the
temporal evolution of sub-events, and does action classification.
  \item We propose a novel, moving actor and object tracking mechanism, called \textit{motion tubes},
which enables the system to track each actor or object independently, and model the motion patterns individually over a long time period.
This allows the system to model actions occurring in a video in micro level, and use these learned dynamics
at a high level afterwards.
 \item We propose three novel, simple, and an efficient mathematical models for fusing two vectors,
in two different domains, based on Choleskey transformation, variance ratio of motion and static vectors, and PCA. \hl{The first two methods} provide
the ability to govern the contribution of each domain for the fused vector precisely and find the optimum contribution of the two domains, mathematically or empirically. Using this advantage, we prove that both static and motion information are complementary and vital for activity recognition through experiments.
%Also, we show that the optimum contribution may vary depending on the data set, and affects the final accuracy significantly.
 \item We prove that the final recognition accuracy depends on the ratio of contribution of static and motion domains. Also we show that
 this optimum ratio depends on the data set. Hence, it is beneficial to exploit this optimum ratio for a better accuracy.
  \item We model the underlying temporal evolution of sub-events for complex activity recognition using an LSTM network. We experimentally
prove that capturing these dynamics indeed benefits the final accuracy.
 \end{itemize}

With the proposed technique
%, and novel techniques,
we outperform the existing best results for the popular data sets UCF-11 \cite{liu2009recognizing}
and Hollywood2 \cite{marszalek2009actions}.


