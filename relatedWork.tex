
There has not been many approaches in activity recognition, which highlight the
importance of exclusively engineered static and motion features. Most of the work
rely on generating spatio-temporal interest regions, such as, action tubes \cite{gkioxari2015finding},
tubelets \cite{jain2014action}, dense trajectory based methods \cite{van2015apt, wang2015action},
spatio-temporal extensions of spatial pyramid pooling \cite{laptev2008learning},
or spatio-temporal low level features  \cite{schuldt2004recognizing, ke2005efficient,shechtman2005space, wang2011action, klaser2008spatio, yu2010real}. Action tubes \cite{gkioxari2015finding} is quite similar to
our motion tubes, but our motion candidate regions are chosen based on more powerful dense trajectories \cite{wang2011action} instead of
raw optic flows. Also, we employ a tracking mechanism of each moving area through motion tubes isolating actions
of each actor throughout the video. Our static interest regions are independent from motion,
unlike in Gkioxari \textit{et al.}~\cite{gkioxari2015finding}, where we are able to extract background scenery information using CNNs, for
action recognition.
A common attribute of these methods is that motion density is the
dominant factor for identifying candidate regions.
In contrast, we treat motion and static features
as two independent domains, and eliminate the dominance factor.

A few attempts has recently been made on exclusive crafting and late fusion
of motion and static features. Simonyan \textit{et al.}~\cite{simonyan2014two} first decomposes a video in to
spatial and temporal components based on RGB and optical flow frames.
Then they apply two deep CNNs on these two components separately to extract spatial and
temporal information. Each network operates mutually exclusively and performs action classification
independently. Afterwards, softmax scores are coalesced by late fusion.

Work done in Feichtenhofer \textit{et al.}\cite{feichtenhofer2016convolutional} is also similar. Instead of late fusion,
they fuse the two domains in a convolutional layer. Both these approaches rely explicitly on automatic feature
generation in increasingly abstract layers. While this has provided promising results on static feature generation,
we argue that motion patterns can be better extracted by hand-crafted features. This is because
temporal dynamics extend to a longer motion duration unlike spatial variations. It is not possible
to capture and discriminate motion patterns in to classes by a system which has a smaller temporal support. There are models
which employ 3D convolution \cite{ji20133d, tran2015learning}, which extends the traditional CNNs into temporal domain.
Ramasinghe \textit{et al.}\cite{7486474} apply CNNs on optic flows, and Kim \textit{et al.}~\cite{kim2007human} on low level hand-crafted inputs
(spatio-temporal outer boundaries volumes), to extract motion features. However, even by generating hierarchical
features on top of pixel level features, it is not easy to discriminate motion classes as the duration extent is short.
Also, tracking and modeling actions of each actor separately in longer time durations is not possible with these
approaches. Our motion features, on the other hand, are capable of capturing motion patterns in longer temporal durations.
Furthermore, with the aid of \textit{motion tubes} our system tracks and models the activities of each moving area separately.

Regarding video evolution, Fernando \textit{et al.}~\cite{fernando2015modeling} postulate
a function capable of ordering the frames of a video
temporally. They learn a ranking function per video using a ranking machine and use the learned parameters as
a video descriptor. Several other methodologies, e.g., HMM \cite{wang2011hidden, wu2014leveraging},
CRF-based methods \cite{song2013action}, also have been employed in this aspect. These methods model the video evolution in frame
level. In contrast, attempts for temporal ordering of atomic events also has been made \cite{rohrbach2012script, bhattacharya2014recognition}.
Rohrbach \textit{et al.}\cite{rohrbach2012script}, encode transition probabilities of a series of events statistically with a HMM model.
Bhattacharya \textit{et al.}\cite{bhattacharya2014recognition} identify low level actions using dense trajectories and assign concept identity
probabilities for each action. They apply a LDS on these generated concept vectors to exploit temporal
dynamics of the low level actions. Li \textit{et al.}\cite{li2013recognizing} uses simple dynamical systems \cite{jackson1992perspectives},
\cite{kailath1974view} to create a dictionary of low-level spatio-temporal attributes. They use these attributes
later as a histogram to represent high level actions. Our method too follows a similar approach,
as we also generate descriptors for sub-events and then extract temporal progression
of these sub-events. However, instead of a simple statistical model, which has a finite dynamic response,
we use an LSTM network \cite{hochreiter1997long} to capture these dynamics. In action recognition literature,
such models are starting appear. In Yue-Hei \textit{et al.}
\cite{yue2015beyond} the LSTM network models the dynamics of the CNN activations, and in Donahue \textit{et al.}\cite{donahue2015long},
the LSTM network learns the temporal dynamics of low level features generated by a CNN.

