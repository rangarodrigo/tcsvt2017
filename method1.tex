
\subsection{Overview}

This section outlines our approach. The overall methodology is illustrated in Fig. \ref{fi:overall}.

%Both static information and motion information, are correlated with the actions occurring in a video. For example, diving action
%is highly associated with swimming pools, and javelin throw is highly likely to happen in a scenario, where a sports field, a javelin and an athlete is
%present. Therefore, it is important to extract both static and motion information, in order to accurately predict and classify actions. In other words,
%individual objects, scene information, and actions and movements of actors and objects are extracted, in the process of prediction.


%Also, a complex action may consist of smaller sub actions. For example, javelin throw action consists of sub events like getting ready,
%running, and throwing the javelin. Also, a specific order of sub events are associated with the complex action.
%Therefore, it is important to investigate the evolution of sub activities with respect to time, in order to predict
%complex actions. We investigate this using following method. A video is segmented in to smaller segments, and features are engineered for each of these sub segments. A video can be then represented as a vector time series $C = [c_{t_0}, c_{t_1}, ...c_{t_{n-1}}]$.
%, where $n$ is the number of segments. Then, a recurrent neural net is applied on these features, to extract temporal dynamics.
\begin{figure*}
  \centering
  %\includegraphics[width=18cm, height=6cm]{overall.pdf}
  \input{figures/overall}
  \caption{\textbf{Overall methodology}. The whole process consists of five major steps: (i) segmenting a video (ii) crafting static features, (iii) crafting motion features,
  (iv) fusing static and motion features, and (v) capturing temporal evolution of sub events. Static and motion features are independent
  and complementary. We generate static features based on  a pre-trained CNN and motion features based on 
  motion tubes, and capture the temporal evolution of sub events using an LSTM network.}

 \label{fi:overall}
\end{figure*}


Our activity classifier classifies video snippets based on their descriptors. In order to compute descriptors, initially, we segment a video into small snippets of 15 frames with a constant frame overlap.
Then we carry out feature construction pipelining for each of these snippets, as shown in Fig. \ref{fi:overall}.
We compute features for each snippet that describe both motion and static domains. 
For extracting motion features we create \textit{motion tubes} across frames, where we track each moving area across the frames using ``action boxes''.
Action boxes are square regions, which exhibit significant motion in each frame. We choose candidate areas by first creating dense trajectories for each frame,
and then clustering trajectory points preceded by a significant amount of pre-processing. This process is explained in sub-section \ref{ss:motion_features}.
These action boxes create motion tubes by getting linking across the frames. Then we calculate HOOF~\cite{chaudhry2009histograms} features within these motion tubes and apply a
bag-of-features method on these to create a motion descriptor for each video segment.

For extracting static features we train a deep CNN on ImageNet. Then we apply this CNN
on the frames of each video snippet to retrieve deep features---output vector from the final softmax layer of the CNN---from it. Then we use these features
to create a static descriptor for the video segment. Afterwards, we combine these motion and static descriptors using one of the fusion models described in sub-section \ref{ss:fusing_static_and_motion}.

The system can then represent a video as a vector time series, $C = [c_{t_0}, c_{t_1}, \dots, c_{t_{n-1}}]$,
where $n$ is the number of segments. Then we apply an LSTM network on these features and exploit the dynamics of time evolution of the combined vector.
Finally, we classify the dynamics of this time series and predict actions.



